{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For all the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def already_processed(ticker: str, out_file: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a given ticker already exists in the output_parquet.\n",
    "    Returns True if the ticker is found, False otherwise.\n",
    "    \"\"\"\n",
    "    if not out_file.exists():\n",
    "        # If output file doesn't exist yet, obviously not processed\n",
    "        return False\n",
    "    \n",
    "    # Read only the ticker column to save memory\n",
    "    df_ticker = pl.read_parquet(str(out_file), columns=[\"ticker\"]).unique()\n",
    "    return ticker in df_ticker[\"ticker\"].to_list()\n",
    "\n",
    "\n",
    "def compute_response_function(df: pl.DataFrame, tau_max: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the response function R(tau) for all lags up to tau_max.\n",
    "    Expects df to have columns: 'bid-price', 'ask-price', 'trade-price'.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): DataFrame with 'bid-price', 'ask-price', 'trade-price'.\n",
    "        tau_max (int): Maximum lag for which to compute the response function.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of response values for each tau from 1 to tau_max.\n",
    "    \"\"\"\n",
    "    # Compute mid-price\n",
    "    df = df.with_columns(\n",
    "        ((pl.col(\"bid-price\") + pl.col(\"ask-price\")) / 2).alias(\"mid-price\")\n",
    "    )\n",
    "\n",
    "    # Compute the sign s = sign(trade-price - mid-price), fill trade-price nulls with 0\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"trade-price\").fill_null(0) - pl.col(\"mid-price\")).sign().alias(\"s\")\n",
    "    )\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    m = df[\"mid-price\"].to_numpy()\n",
    "    s = df[\"s\"].to_numpy()\n",
    "\n",
    "    # Ensure tau_max does not exceed data length\n",
    "    tau_max = min(tau_max, len(m) - 1)\n",
    "    response = []\n",
    "\n",
    "    for tau in range(1, tau_max + 1):\n",
    "        # Only compute if we have enough data after shifting\n",
    "        if len(m[tau:]) > 0:\n",
    "            shifted_diff = m[tau:] - m[:-tau]  # m_{n+tau} - m_n\n",
    "            resp = np.mean(s[:-tau] * shifted_diff)\n",
    "            response.append(resp)\n",
    "        else:\n",
    "            response.append(np.nan)\n",
    "\n",
    "    return np.array(response)\n",
    "\n",
    "\n",
    "def compute_response_by_date_for_folder(\n",
    "    folder_path: str,\n",
    "    tau_max: int,\n",
    "    output_parquet: str\n",
    "):\n",
    "    \"\"\"\n",
    "    For each *.parquet in folder_path:\n",
    "      - If the ticker is already in output_parquet, skip it.\n",
    "      - Otherwise, group by date, compute the response function, and append results to output_parquet.\n",
    "\n",
    "    The output file will have columns: [ticker, date, tau, response].\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    out_file = Path(output_parquet)\n",
    "\n",
    "    # Iterate over all parquet files in the folder\n",
    "    for file_path in folder.glob(\"*.parquet\"):\n",
    "        ticker = file_path.stem  # e.g., \"AAPL.OQ_combined\"\n",
    "\n",
    "        # Check if we already have this ticker in the final parquet\n",
    "        if already_processed(ticker, out_file):\n",
    "            print(f\"Skipping {ticker}, already processed.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {ticker}...\")\n",
    "\n",
    "        # Read in the data\n",
    "        df = pl.read_parquet(str(file_path))\n",
    "\n",
    "        # Gather results in a Python list of dicts\n",
    "        results = []\n",
    "\n",
    "        # Get all unique dates\n",
    "        unique_dates = df[\"date\"].unique().to_list()\n",
    "\n",
    "        for date_val in unique_dates:\n",
    "            # Filter to just that date\n",
    "            group_df = df.filter(pl.col(\"date\") == date_val)\n",
    "            if group_df.is_empty():\n",
    "                continue\n",
    "\n",
    "            # Compute the response function\n",
    "            response_arr = compute_response_function(group_df, tau_max)\n",
    "\n",
    "            # Collect each tau's response\n",
    "            for tau_idx, resp_val in enumerate(response_arr, start=1):\n",
    "                results.append({\n",
    "                    \"ticker\": ticker,\n",
    "                    \"date\": date_val,\n",
    "                    \"tau\": tau_idx,\n",
    "                    \"response\": resp_val,\n",
    "                })\n",
    "\n",
    "        # Skip writing if no results (shouldn't happen if the file had data)\n",
    "        if not results:\n",
    "            print(f\"No valid response data for {ticker}.\")\n",
    "            continue\n",
    "\n",
    "        # Convert to Polars DataFrame\n",
    "        final_df = pl.DataFrame(results)\n",
    "\n",
    "        # If out_file doesn't exist, just write it\n",
    "        if not out_file.exists():\n",
    "            final_df.write_parquet(str(out_file))\n",
    "        else:\n",
    "            # If it does exist, read it and append\n",
    "            existing_df = pl.read_parquet(str(out_file))\n",
    "            combined = pl.concat([existing_df, final_df], how=\"vertical\")\n",
    "            combined.write_parquet(str(out_file))\n",
    "\n",
    "        print(f\"Finished {ticker}, appended to {out_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AAPL.OQ_combined...\n",
      "Processing AMGN.OQ_combined...\n",
      "Processing AXP.N_combined...\n",
      "Processing BA.N_combined...\n",
      "Processing CAT.N_combined...\n",
      "Processing CSCO.OQ_combined...\n",
      "Processing CVX.N_combined...\n",
      "Processing DOW.N_combined...\n",
      "Processing GS.N_combined...\n",
      "Processing HD.N_combined...\n",
      "Processing IBM.N_combined...\n",
      "Processing INTC.OQ_combined...\n",
      "Processing JNJ.N_combined...\n",
      "Processing JPM.N_combined...\n",
      "Processing KO.N_combined...\n",
      "Processing MCD.N_combined...\n",
      "Processing MMM.N_combined...\n",
      "Processing MRK.N_combined...\n",
      "Processing MSFT.OQ_combined...\n",
      "Processing NKE.N_combined...\n",
      "Processing PFE.N_combined...\n",
      "Processing PG.N_combined...\n",
      "Processing TRV.N_combined...\n",
      "Processing UNH.N_combined...\n",
      "Processing UTX.N_combined...\n",
      "Processing V.N_combined...\n",
      "Processing VZ.N_combined...\n",
      "Processing WMT.N_combined...\n",
      "Processing XOM.N_combined...\n",
      "Done! Results written to response_functions_all_stocks.parquet\n"
     ]
    }
   ],
   "source": [
    "compute_response_by_date_for_folder(\n",
    "        folder_path=\"data\",\n",
    "        tau_max=1000,\n",
    "        output_parquet=\"response_functions_all_stocks.parquet\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_percentile_threshold(\n",
    "    response_parquet: str, \n",
    "    tau_max: int, \n",
    "    percentile: float = 99.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Determines the threshold based on a specified percentile of the max_response values.\n",
    "    This implementation avoids using `groupby` by utilizing window functions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    response_parquet : str\n",
    "        Path to the Parquet file with columns [ticker, date, tau, response].\n",
    "    tau_max : int\n",
    "        The maximum lag to consider.\n",
    "    percentile : float\n",
    "        The percentile to use for thresholding (e.g., 99.0 for the 99th percentile).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The computed threshold value.\n",
    "    \"\"\"\n",
    "    # 1. Read the response data\n",
    "    try:\n",
    "        df = pl.read_parquet(response_parquet)\n",
    "        print(f\"Successfully read {response_parquet}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Failed to read {response_parquet}: {e}\")\n",
    "\n",
    "    # 4. Filter by tau_max\n",
    "    df_filtered = df.filter(pl.col(\"tau\") <= tau_max)\n",
    "\n",
    "    # 5. Compute max_response per (ticker, date) using window functions\n",
    "    df_with_max = df_filtered.with_columns(\n",
    "        pl.col(\"response\").max().over([\"ticker\", \"date\"]).alias(\"max_response\")\n",
    "    )\n",
    "\n",
    "    # 6. Extract unique (ticker, date, max_response) combinations\n",
    "    df_daily_max = df_with_max.select([\"ticker\", \"date\", \"max_response\"]).unique()\n",
    "    print(f\"Daily Max DataFrame shape: {df_daily_max.shape}\")\n",
    "\n",
    "    # 7. Calculate the desired percentile\n",
    "    try:\n",
    "        threshold = df_daily_max[\"max_response\"].quantile(percentile / 100.0)\n",
    "        print(f\"Determined threshold at the {percentile}th percentile: {threshold}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to compute quantile: {e}\")\n",
    "\n",
    "    return threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_flash_crashes_from_response_file(\n",
    "    response_parquet: str, \n",
    "    tau_max: int, \n",
    "    threshold: float, \n",
    "    output_parquet: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file with precomputed response functions, detects potential flash-crash\n",
    "    days by looking at the maximum response for each (ticker, date), and writes a filtered \n",
    "    Parquet of flagged days.\n",
    "\n",
    "    This implementation avoids using `groupby` by utilizing window functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response_parquet : str\n",
    "        Path to the Parquet file that has columns [ticker, date, tau, response].\n",
    "    tau_max : int\n",
    "        The maximum lag to consider; rows with tau > tau_max will be ignored.\n",
    "    threshold : float\n",
    "        The cut-off above which we label a day as having a potential flash crash.\n",
    "    output_parquet : str\n",
    "        Path to the Parquet file in which to store flash-crash detections. This file \n",
    "        will have columns [ticker, date, max_response].\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df = pl.read_parquet(response_parquet)\n",
    "        print(f\"Successfully read {response_parquet}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Failed to read {response_parquet}: {e}\")\n",
    "\n",
    "    # 4. Filter by tau_max\n",
    "    df_filtered = df.filter(pl.col(\"tau\") <= tau_max)\n",
    "\n",
    "    # 5. Compute max_response per (ticker, date) using window functions\n",
    "    df_with_max = df_filtered.with_columns(\n",
    "        pl.col(\"response\").max().over([\"ticker\", \"date\"]).alias(\"max_response\")\n",
    "    )\n",
    "    # 6. Extract unique (ticker, date, max_response) combinations\n",
    "    df_daily_max = df_with_max.select([\"ticker\", \"date\", \"max_response\"]).unique()\n",
    "    # 7. Filter days where max_response exceeds the threshold\n",
    "    df_flash_crash = df_daily_max.filter(pl.col(\"max_response\") > threshold)\n",
    "\n",
    "    # 8. Write the flagged days to the output Parquet file\n",
    "    if df_flash_crash.is_empty():\n",
    "        print(\"No potential flash-crash days found. Writing empty file.\")\n",
    "    else:\n",
    "        print(f\"Writing flash-crash detections to {output_parquet} ...\")\n",
    "\n",
    "    df_flash_crash.write_parquet(output_parquet)\n",
    "    print(\"Flash-crash detection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5754000, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pl.read_parquet(\"response_functions_all_stocks.parquet\")\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read response_functions_all_stocks.parquet\n",
      "Daily Max DataFrame shape: (5754, 3)\n",
      "Determined threshold at the 99.0th percentile: 0.008211838832022249\n"
     ]
    }
   ],
   "source": [
    "tau_max = 100  \n",
    "percentile = 99.0  # Starting with the 99th percentile\n",
    "threshold = determine_percentile_threshold(\"response_functions_all_stocks.parquet\", tau_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read response_functions_all_stocks.parquet\n",
      "Writing flash-crash detections to flash_crashes_detected.parquet ...\n",
      "Flash-crash detection completed.\n"
     ]
    }
   ],
   "source": [
    "detect_flash_crashes_from_response_file(\n",
    "        response_parquet=\"response_functions_all_stocks.parquet\", \n",
    "        tau_max=100,            # or any integer limit on tau\n",
    "        threshold=threshold,         # or any threshold that suits your detection logic\n",
    "        output_parquet=\"flash_crashes_detected.parquet\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (58, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ticker</th><th>date</th><th>max_response</th></tr><tr><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;AAPL.OQ_combined&quot;</td><td>&quot;2010-11-03&quot;</td><td>0.011162</td></tr><tr><td>&quot;IBM.N_combined&quot;</td><td>&quot;2010-01-19&quot;</td><td>NaN</td></tr><tr><td>&quot;AAPL.OQ_combined&quot;</td><td>&quot;2010-11-29&quot;</td><td>0.012902</td></tr><tr><td>&quot;AAPL.OQ_combined&quot;</td><td>&quot;2010-06-08&quot;</td><td>0.011305</td></tr><tr><td>&quot;GS.N_combined&quot;</td><td>&quot;2010-10-26&quot;</td><td>NaN</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;WMT.N_combined&quot;</td><td>&quot;2010-12-15&quot;</td><td>NaN</td></tr><tr><td>&quot;AAPL.OQ_combined&quot;</td><td>&quot;2010-12-07&quot;</td><td>0.010424</td></tr><tr><td>&quot;JPM.N_combined&quot;</td><td>&quot;2010-01-19&quot;</td><td>NaN</td></tr><tr><td>&quot;AAPL.OQ_combined&quot;</td><td>&quot;2010-09-30&quot;</td><td>0.009613</td></tr><tr><td>&quot;AAPL.OQ_combined&quot;</td><td>&quot;2010-11-15&quot;</td><td>0.011755</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (58, 3)\n",
       "┌──────────────────┬────────────┬──────────────┐\n",
       "│ ticker           ┆ date       ┆ max_response │\n",
       "│ ---              ┆ ---        ┆ ---          │\n",
       "│ str              ┆ str        ┆ f64          │\n",
       "╞══════════════════╪════════════╪══════════════╡\n",
       "│ AAPL.OQ_combined ┆ 2010-11-03 ┆ 0.011162     │\n",
       "│ IBM.N_combined   ┆ 2010-01-19 ┆ NaN          │\n",
       "│ AAPL.OQ_combined ┆ 2010-11-29 ┆ 0.012902     │\n",
       "│ AAPL.OQ_combined ┆ 2010-06-08 ┆ 0.011305     │\n",
       "│ GS.N_combined    ┆ 2010-10-26 ┆ NaN          │\n",
       "│ …                ┆ …          ┆ …            │\n",
       "│ WMT.N_combined   ┆ 2010-12-15 ┆ NaN          │\n",
       "│ AAPL.OQ_combined ┆ 2010-12-07 ┆ 0.010424     │\n",
       "│ JPM.N_combined   ┆ 2010-01-19 ┆ NaN          │\n",
       "│ AAPL.OQ_combined ┆ 2010-09-30 ┆ 0.009613     │\n",
       "│ AAPL.OQ_combined ┆ 2010-11-15 ┆ 0.011755     │\n",
       "└──────────────────┴────────────┴──────────────┘"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = pl.read_parquet(\"flash_crashes_detected.parquet\")\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL = pl.read_parquet(\"data/AAPL.OQ_combined.parquet\")\n",
    "AMGN = pl.read_parquet(\"data/AMGN.OQ_combined.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hayashi-Yoshida Covariance: 4.6598161306899626e-07\n",
      "Variance of X: 3.0557134919094433e-06\n",
      "Variance of Y: 5.604403826875911e-06\n",
      "Correlation: 0.1126024277275251\n",
      "\n",
      "Hayashi-Yoshida Covariance: -1.796399056251842e-06\n",
      "Variance of X: 0.00019806774824034315\n",
      "Variance of Y: 2.6253316839041454e-05\n",
      "Correlation: -0.024911727585088677\n",
      "\n",
      "Hayashi-Yoshida Covariance: 3.3933135538271396e-08\n",
      "Variance of X: 8.264664089038107e-06\n",
      "Variance of Y: 1.1241841328063137e-05\n",
      "Correlation: 0.003520405273266799\n"
     ]
    }
   ],
   "source": [
    "target_date = \"2010-05-06\"\n",
    "start_time = \"14:45\"\n",
    "AAPL_flash_crash_day = filter_by_date(AAPL, \"2010-05-06\")\n",
    "AMGN_flash_crash_day = filter_by_date(AMGN, \"2010-05-06\")\n",
    "\n",
    "AAPL_flash_crash_day = AAPL_flash_crash_day.with_columns(\n",
    "    pl.col('index').str.slice(11, 12).alias('truncated_index')  # Extract \"HH:MM:SS.ssssss\"\n",
    ")\n",
    "AMGN_flash_crash_day = AMGN_flash_crash_day.with_columns(\n",
    "    pl.col('index').str.slice(11, 12).alias('truncated_index')\n",
    ")\n",
    "\n",
    "AAPL_flash_crash_day = AAPL_flash_crash_day.with_columns(((AAPL_flash_crash_day['bid-price'] + AAPL_flash_crash_day['ask-price']) / 2).alias('mid_price'))\n",
    "AMGN_flash_crash_day = AMGN_flash_crash_day.with_columns(((AMGN_flash_crash_day['bid-price'] + AMGN_flash_crash_day['ask-price']) / 2).alias('mid_price'))\n",
    "\n",
    "AAPL_flash_crash_day = AAPL_flash_crash_day.with_columns(AAPL_flash_crash_day['mid_price'].pct_change().alias('mid_price_return'))\n",
    "AMGN_flash_crash_day = AMGN_flash_crash_day.with_columns(AMGN_flash_crash_day['mid_price'].pct_change().alias('mid_price_return'))\n",
    "\n",
    "# Remove rows where returns are zero or NaN\n",
    "AAPL_flash_crash_day = AAPL_flash_crash_day.filter(AAPL_flash_crash_day['mid_price_return'] != 0).drop_nulls()\n",
    "AMGN_flash_crash_day = AMGN_flash_crash_day.filter(AMGN_flash_crash_day['mid_price_return'] != 0).drop_nulls()\n",
    "\n",
    "# 3. Join the dataframes with full join and sort\n",
    "result_pl_df = AAPL_flash_crash_day.join(AMGN_flash_crash_day, on='truncated_index', how='full').sort('truncated_index')\n",
    "\n",
    "# Forward-fill directly on the existing columns\n",
    "result_pl_df_filled = result_pl_df.with_columns([\n",
    "    pl.col('mid_price').fill_null(strategy='forward'),\n",
    "    pl.col('mid_price_right').fill_null(strategy='forward')\n",
    "])\n",
    "\n",
    "df_before, df_during, df_after = split_dataframe_by_quarter(result_pl_df_filled, target_date, start_time )\n",
    "\n",
    "# Call the function on your dataframe\n",
    "result = hayashi_yoshida_covariance(df_before)\n",
    "result_2 = hayashi_yoshida_covariance(df_during)\n",
    "result_3 = hayashi_yoshida_covariance(df_after)\n",
    "# Print results\n",
    "print(f\"Hayashi-Yoshida Covariance: {result['hayashi_yoshida_covariance']}\")\n",
    "print(f\"Variance of X: {result['variance_x']}\")\n",
    "print(f\"Variance of Y: {result['variance_y']}\")\n",
    "print(f\"Correlation: {result['correlation']}\")\n",
    "print()\n",
    "# Print results\n",
    "print(f\"Hayashi-Yoshida Covariance: {result_2['hayashi_yoshida_covariance']}\")\n",
    "print(f\"Variance of X: {result_2['variance_x']}\")\n",
    "print(f\"Variance of Y: {result_2['variance_y']}\")\n",
    "print(f\"Correlation: {result_2['correlation']}\")\n",
    "print()\n",
    "# Print results\n",
    "print(f\"Hayashi-Yoshida Covariance: {result_3['hayashi_yoshida_covariance']}\")\n",
    "print(f\"Variance of X: {result_3['variance_x']}\")\n",
    "print(f\"Variance of Y: {result_3['variance_y']}\")\n",
    "print(f\"Correlation: {result_3['correlation']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Tuple\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_date(df: pl.DataFrame, target_date: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the Polars DataFrame by the given date string\n",
    "    and returns a new DataFrame.\n",
    "    \"\"\"\n",
    "    return df.filter(pl.col(\"date\") == target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intervaltree import Interval, IntervalTree\n",
    "\n",
    "def hayashi_yoshida_covariance(dataframe, default_columns=(\"mid_price_return\", \"mid_price_return_right\")):\n",
    "    \"\"\"\n",
    "    Calculate the Hayashi-Yoshida covariance estimator for two numeric columns in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pl.DataFrame): The input dataframe.\n",
    "        default_columns (tuple): A tuple specifying default column names for the calculation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the Hayashi-Yoshida covariance, variances, and correlation.\n",
    "    \"\"\"\n",
    "    if all(col in dataframe.columns for col in default_columns):\n",
    "        col_x, col_y = default_columns\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"The dataframe must contain the specified columns: {default_columns}\"\n",
    "        )\n",
    "\n",
    "    # Convert Polars DataFrame to Pandas for interval handling\n",
    "    df = dataframe.to_pandas()\n",
    "\n",
    "    # Extract necessary columns and drop NaNs\n",
    "    df_x = df[[\"truncated_index\", col_x]].dropna().sort_values(by=\"truncated_index\").drop_duplicates(subset=\"truncated_index\")\n",
    "    df_y = df[[\"truncated_index_right\", col_y]].dropna().sort_values(by=\"truncated_index_right\").drop_duplicates(subset=\"truncated_index_right\")\n",
    "\n",
    "    # Parse truncated_index into proper datetime with millisecond precision\n",
    "    df_x[\"truncated_index\"] = pd.to_datetime(df_x[\"truncated_index\"], format=\"%H:%M:%S.%f\")\n",
    "    df_y[\"truncated_index_right\"] = pd.to_datetime(df_y[\"truncated_index_right\"], format=\"%H:%M:%S.%f\")\n",
    "\n",
    "    # Create intervals for both series\n",
    "    intervals_x = pd.IntervalIndex.from_arrays(\n",
    "        df_x[\"truncated_index\"][:-1], df_x[\"truncated_index\"][1:], closed=\"right\"\n",
    "    )\n",
    "    intervals_y = pd.IntervalIndex.from_arrays(\n",
    "        df_y[\"truncated_index_right\"][:-1], df_y[\"truncated_index_right\"][1:], closed=\"right\"\n",
    "    )\n",
    "\n",
    "    # Build an interval tree for Y\n",
    "    tree_y = IntervalTree(\n",
    "        Interval(begin.value, end.value, idx)\n",
    "        for idx, (begin, end) in enumerate(zip(df_y[\"truncated_index_right\"][:-1], df_y[\"truncated_index_right\"][1:]))\n",
    "    )\n",
    "\n",
    "    # Calculate covariance\n",
    "    # Calculate average of the product of returns\n",
    "    covariance = 0\n",
    "    #count = 0  # To keep track of the number of products\n",
    "    for i, interval_x in enumerate(intervals_x):\n",
    "        overlaps = tree_y.overlap(interval_x.left.value, interval_x.right.value)  # Use overlap instead of search\n",
    "        for overlap in overlaps:\n",
    "            j = overlap.data  # Index in df_y\n",
    "            covariance += df_x[col_x].iloc[i] * df_y[col_y].iloc[j]\n",
    "            #count += 1\n",
    "\n",
    "    # Calculate average\n",
    "    #average_product = covariance / count if count > 0 else 0\n",
    "\n",
    "        \n",
    "    # Calculate variances\n",
    "    variance_x = np.sum(df_x[col_x] ** 2)\n",
    "    variance_y = np.sum(df_y[col_y] ** 2)\n",
    "\n",
    "    # Calculate correlation\n",
    "    correlation = covariance / (np.sqrt(variance_x * variance_y) +1e-8)\n",
    "\n",
    "    return {\n",
    "        \"hayashi_yoshida_covariance\": average_product,\n",
    "        \"variance_x\": variance_x,\n",
    "        \"variance_y\": variance_y,\n",
    "        \"correlation\": correlation,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe_by_quarter(df: pl.DataFrame, date: str, start_time_str: str) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    clean_df = df.filter(\n",
    "        (pl.col('mid_price_return').is_not_null()) & \n",
    "        (pl.col('mid_price_return_right').is_not_null())\n",
    "    )\n",
    "    \n",
    "    start_datetime_str = f\"{date} {start_time_str}:00\"\n",
    "    start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    quarter_duration = timedelta(minutes=15)\n",
    "    end_datetime = start_datetime + quarter_duration\n",
    "    \n",
    "    start_quarter = start_datetime.strftime(\"%H:%M:%S\")\n",
    "    end_quarter = end_datetime.strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    df_before_quarter = clean_df.filter(pl.col(\"truncated_index\") < start_quarter)\n",
    "    df_during_quarter = clean_df.filter(\n",
    "        (pl.col(\"truncated_index\") >= start_quarter) & \n",
    "        (pl.col(\"truncated_index\") <= end_quarter)\n",
    "    )\n",
    "    df_after_quarter = clean_df.filter(pl.col(\"truncated_index\") > end_quarter)\n",
    "    \n",
    "    return df_before_quarter, df_during_quarter, df_after_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pair(asset1_file, asset2_file, date, start_time):\n",
    "    \"\"\"\n",
    "    Process two parquet files to compute the Hayashi-Yoshida covariance, variances, and correlations.\n",
    "\n",
    "    Parameters:\n",
    "        asset1_file (str): Path to the first asset's parquet file.\n",
    "        asset2_file (str): Path to the second asset's parquet file.\n",
    "        date (str): The date to filter on (e.g., \"2010-05-06\").\n",
    "        start_time (str): The start time of the quarter (e.g., \"09:30\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results for each period (before, during, after).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the parquet files\n",
    "    asset1 = pl.read_parquet(asset1_file)\n",
    "    asset2 = pl.read_parquet(asset2_file)\n",
    "\n",
    "    # Filter by date\n",
    "    asset1 = filter_by_date(asset1, date)\n",
    "    asset2 = filter_by_date(asset2, date)\n",
    "\n",
    "    # Add truncated index\n",
    "    asset1 = asset1.with_columns(pl.col('index').str.slice(11, 12).alias('truncated_index'))\n",
    "    asset2 = asset2.with_columns(pl.col('index').str.slice(11, 12).alias('truncated_index'))\n",
    "\n",
    "    # Compute mid-price and mid-price returns\n",
    "    asset1 = asset1.with_columns(((asset1['bid-price'] + asset1['ask-price']) / 2).alias('mid_price'))\n",
    "    asset1 = asset1.with_columns(asset1['mid_price'].pct_change().alias('mid_price_return'))\n",
    "    asset1 = asset1.filter(asset1['mid_price_return'] != 0).drop_nulls()\n",
    "\n",
    "    asset2 = asset2.with_columns(((asset2['bid-price'] + asset2['ask-price']) / 2).alias('mid_price'))\n",
    "    asset2 = asset2.with_columns(asset2['mid_price'].pct_change().alias('mid_price_return'))\n",
    "    asset2 = asset2.filter(asset2['mid_price_return'] != 0).drop_nulls()\n",
    "\n",
    "    # Remove rows where returns are zero or NaN\n",
    "    asset1 = asset1.filter(asset1['mid_price_return'] != 0).drop_nulls()\n",
    "    asset2 = asset2.filter(asset2['mid_price_return'] != 0).drop_nulls()\n",
    "\n",
    "\n",
    "    # Join the dataframes\n",
    "    result_df = asset1.join(asset2, on='truncated_index', how='full').sort('truncated_index')\n",
    "\n",
    "    # Forward fill\n",
    "    result_df = result_df.with_columns([\n",
    "        pl.col('mid_price').fill_null(strategy='forward'),\n",
    "        pl.col('mid_price_right').fill_null(strategy='forward')\n",
    "    ])\n",
    "\n",
    "    # Split the dataframe into periods\n",
    "    df_before, df_during, df_after = split_dataframe_by_quarter(result_df, date, start_time)\n",
    "\n",
    "    # Compute covariance and variances for each period\n",
    "    results = {\n",
    "        \"before\": hayashi_yoshida_covariance(df_before),\n",
    "        \"during\": hayashi_yoshida_covariance(df_during),\n",
    "        \"after\": hayashi_yoshida_covariance(df_after),\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL the files covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = process_pair(\"data_clean/AAPL.OQ_combined.parquet\",\"data_clean/AMGN.OQ_combined.parquet\",\"2010-05-06\",\"14:45\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance_matrices(data_folder, dates, output_folder):\n",
    "    \"\"\"\n",
    "    Compute and append covariance matrices for all asset pairs across multiple dates.\n",
    "\n",
    "    Parameters:\n",
    "        data_folder (str): Path to the folder containing asset parquet files.\n",
    "        dates (list[str]): List of dates and times to process (e.g., [\"2010-05-06-14:45\"]).\n",
    "        output_folder (str): Path to the folder to save covariance matrices as parquet files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # List all parquet files in the data folder\n",
    "    asset_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "    # Extract asset names from filenames\n",
    "    asset_names = [os.path.basename(f).split('.')[0] for f in asset_files]\n",
    "    n_assets = len(asset_names)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize or read existing parquet files for each period\n",
    "    def initialize_or_read_parquet(file_path, asset_names):\n",
    "        if os.path.exists(file_path):\n",
    "            return pl.read_parquet(file_path)\n",
    "        else:\n",
    "            schema = {\"date\": pl.Utf8}\n",
    "            schema.update({f\"{name}_{name2}\": pl.Float64 for name in asset_names for name2 in asset_names})\n",
    "            return pl.DataFrame(schema=schema)\n",
    "\n",
    "    before_path = os.path.join(output_folder, \"before.parquet\")\n",
    "    during_path = os.path.join(output_folder, \"during.parquet\")\n",
    "    after_path = os.path.join(output_folder, \"after.parquet\")\n",
    "\n",
    "    before_df = initialize_or_read_parquet(before_path, asset_names)\n",
    "    during_df = initialize_or_read_parquet(during_path, asset_names)\n",
    "    after_df = initialize_or_read_parquet(after_path, asset_names)\n",
    "\n",
    "    # Process each date\n",
    "    for datestring in dates:\n",
    "        try:\n",
    "            date, start_time = datestring.rsplit(\"-\", 1)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid date format: {datestring}. Expected format: YYYY-MM-DD-HH:MM\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing date: {date} with start time: {start_time}\")\n",
    "\n",
    "        # Initialize covariance matrices for this date\n",
    "        cov_matrix_before = np.zeros((n_assets, n_assets))\n",
    "        cov_matrix_during = np.zeros((n_assets, n_assets))\n",
    "        cov_matrix_after = np.zeros((n_assets, n_assets))\n",
    "\n",
    "        # Iterate over all asset pairs\n",
    "        for i, j in itertools.combinations_with_replacement(range(n_assets), 2):\n",
    "            asset1_file = asset_files[i]\n",
    "            asset2_file = asset_files[j]\n",
    "\n",
    "            try:\n",
    "                # Compute covariance results for the pair\n",
    "                results = process_pair(asset1_file, asset2_file, date, start_time)\n",
    "\n",
    "                # Fill covariance matrices\n",
    "                cov_matrix_before[i, j] = results[\"before\"][\"hayashi_yoshida_covariance\"]\n",
    "                cov_matrix_during[i, j] = results[\"during\"][\"hayashi_yoshida_covariance\"]\n",
    "                cov_matrix_after[i, j] = results[\"after\"][\"hayashi_yoshida_covariance\"]\n",
    "\n",
    "                # Symmetric matrix: fill the lower triangle\n",
    "                if i != j:\n",
    "                    cov_matrix_before[j, i] = cov_matrix_before[i, j]\n",
    "                    cov_matrix_during[j, i] = cov_matrix_during[i, j]\n",
    "                    cov_matrix_after[j, i] = cov_matrix_after[i, j]\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing pair ({asset_names[i]}, {asset_names[j]}): {e}\")\n",
    "                continue\n",
    "\n",
    "        # Convert covariance matrices to dataframes\n",
    "        date_label = {\"date\": [date] * (n_assets * n_assets)}\n",
    "        col_names = [f\"{name}_{name2}\" for name in asset_names for name2 in asset_names]\n",
    "\n",
    "        before_matrix_df = pl.DataFrame({**date_label, **dict(zip(col_names, cov_matrix_before.flatten()))})\n",
    "        during_matrix_df = pl.DataFrame({**date_label, **dict(zip(col_names, cov_matrix_during.flatten()))})\n",
    "        after_matrix_df = pl.DataFrame({**date_label, **dict(zip(col_names, cov_matrix_after.flatten()))})\n",
    "\n",
    "        # Append to the respective parquet files\n",
    "        before_df = pl.concat([before_df, before_matrix_df], how=\"vertical\").unique(subset=[\"date\"])\n",
    "        during_df = pl.concat([during_df, during_matrix_df], how=\"vertical\").unique(subset=[\"date\"])\n",
    "        after_df = pl.concat([after_df, after_matrix_df], how=\"vertical\").unique(subset=[\"date\"])\n",
    "\n",
    "    # Write the updated parquet files back\n",
    "    before_df.write_parquet(before_path)\n",
    "    during_df.write_parquet(during_path)\n",
    "    after_df.write_parquet(after_path)\n",
    "\n",
    "    print(f\"Covariance matrices saved for all dates in {output_folder}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date: 2010-05-06 with start time: 14:45\n",
      "Covariance matrices saved for all dates in output_cov_matrices.\n"
     ]
    }
   ],
   "source": [
    "compute_covariance_matrices(\n",
    "    data_folder=\"data_clean\",\n",
    "    dates=[\"2010-05-06-14:45\"],\n",
    "    output_folder=\"output_cov_matrices\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariances (Before):\n",
      "shape: (1, 3)\n",
      "┌────────────┬───────────┬───────────┐\n",
      "│ date       ┆ AAPL_AMGN ┆ MCD_MMM   │\n",
      "│ ---        ┆ ---       ┆ ---       │\n",
      "│ str        ┆ f64       ┆ f64       │\n",
      "╞════════════╪═══════════╪═══════════╡\n",
      "│ 2010-05-06 ┆ 4.6598e-7 ┆ 5.2858e-7 │\n",
      "└────────────┴───────────┴───────────┘\n",
      "\n",
      "Covariances (During):\n",
      "shape: (1, 3)\n",
      "┌────────────┬───────────┬────────────┐\n",
      "│ date       ┆ AAPL_AMGN ┆ MCD_MMM    │\n",
      "│ ---        ┆ ---       ┆ ---        │\n",
      "│ str        ┆ f64       ┆ f64        │\n",
      "╞════════════╪═══════════╪════════════╡\n",
      "│ 2010-05-06 ┆ -0.000002 ┆ -3.7522e-7 │\n",
      "└────────────┴───────────┴────────────┘\n",
      "\n",
      "Covariances (After):\n",
      "shape: (1, 3)\n",
      "┌────────────┬───────────┬───────────┐\n",
      "│ date       ┆ AAPL_AMGN ┆ MCD_MMM   │\n",
      "│ ---        ┆ ---       ┆ ---       │\n",
      "│ str        ┆ f64       ┆ f64       │\n",
      "╞════════════╪═══════════╪═══════════╡\n",
      "│ 2010-05-06 ┆ 3.3933e-8 ┆ 7.9647e-7 │\n",
      "└────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "def read_covariances(parquet_file, asset_pairs, date=None):\n",
    "    \"\"\"\n",
    "    Reads the covariances for specific asset pairs from a parquet file.\n",
    "\n",
    "    Parameters:\n",
    "        parquet_file (str): Path to the parquet file (before, during, or after).\n",
    "        asset_pairs (list[tuple]): List of tuples specifying asset pairs (e.g., [(\"AAPL.OQ\", \"AMGN.OQ\")]).\n",
    "        date (str, optional): If provided, filter results for a specific date (e.g., \"2010-05-06\").\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame containing covariances for the specified asset pairs.\n",
    "    \"\"\"\n",
    "    # Load the parquet file\n",
    "    df = pl.read_parquet(parquet_file)\n",
    "\n",
    "    # Construct the column names for the pairs\n",
    "    pair_columns = [f\"{pair[0]}_{pair[1]}\" for pair in asset_pairs]\n",
    "\n",
    "    # Filter for the specific date if provided\n",
    "    if date:\n",
    "        df = df.filter(pl.col(\"date\") == date)\n",
    "\n",
    "    # Select only the relevant columns\n",
    "    columns_to_select = [\"date\"] + pair_columns\n",
    "    result_df = df.select(columns_to_select)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "parquet_folder = \"output_cov_matrices\"\n",
    "asset_pairs = [\n",
    "    (\"AAPL\", \"AMGN\"),\n",
    "    (\"MCD\", \"MMM\"),\n",
    "]\n",
    "date = \"2010-05-06\"\n",
    "\n",
    "# Read covariances for each period\n",
    "before_covariances = read_covariances(f\"{parquet_folder}/before.parquet\", asset_pairs, date)\n",
    "during_covariances = read_covariances(f\"{parquet_folder}/during.parquet\", asset_pairs, date)\n",
    "after_covariances = read_covariances(f\"{parquet_folder}/after.parquet\", asset_pairs, date)\n",
    "\n",
    "# Print the results\n",
    "print(\"Covariances (Before):\")\n",
    "print(before_covariances)\n",
    "\n",
    "print(\"\\nCovariances (During):\")\n",
    "print(during_covariances)\n",
    "\n",
    "print(\"\\nCovariances (After):\")\n",
    "print(after_covariances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Covariance File:\n",
      "{'rows': 1, 'columns': 484, 'n_assets': 22}\n",
      "During Covariance File:\n",
      "{'rows': 1, 'columns': 484, 'n_assets': 22}\n",
      "After Covariance File:\n",
      "{'rows': 1, 'columns': 484, 'n_assets': 22}\n"
     ]
    }
   ],
   "source": [
    "def check_covariance_shapes(output_folder):\n",
    "    \"\"\"\n",
    "    Check the shape of the covariance matrices stored in the parquet files.\n",
    "\n",
    "    Parameters:\n",
    "        output_folder (str): Path to the folder containing the covariance parquet files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the shapes of the covariance matrices.\n",
    "    \"\"\"\n",
    "    file_paths = {\n",
    "        \"before\": f\"{output_folder}/before.parquet\",\n",
    "        \"during\": f\"{output_folder}/during.parquet\",\n",
    "        \"after\": f\"{output_folder}/after.parquet\",\n",
    "    }\n",
    "\n",
    "    shapes = {}\n",
    "\n",
    "    for period, file_path in file_paths.items():\n",
    "        try:\n",
    "            df = pl.read_parquet(file_path)\n",
    "            if len(df) > 0:\n",
    "                # Number of unique assets by splitting column names\n",
    "                asset_columns = [col for col in df.columns if \"_\" in col]\n",
    "                n_assets = int(len(asset_columns) ** 0.5)  # Square root of total columns\n",
    "                shapes[period] = {\"rows\": len(df), \"columns\": len(asset_columns), \"n_assets\": n_assets}\n",
    "            else:\n",
    "                shapes[period] = {\"rows\": 0, \"columns\": 0, \"n_assets\": 0}\n",
    "        except Exception as e:\n",
    "            shapes[period] = {\"error\": str(e)}\n",
    "\n",
    "    return shapes\n",
    "\n",
    "# Example usage\n",
    "output_folder = \"output_cov_matrices\"\n",
    "shapes = check_covariance_shapes(output_folder)\n",
    "\n",
    "# Print the results\n",
    "for period, info in shapes.items():\n",
    "    print(f\"{period.capitalize()} Covariance File:\")\n",
    "    print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
